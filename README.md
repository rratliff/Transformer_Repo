# Transformer Hackathon üöÄ

Build your own GPT-style transformer model from scratch and compete on the leaderboard!

**üèÜ Leaderboard:** [https://huggingface.co/datasets/abhisu30/transformer-hackathon-leaderboard](https://huggingface.co/datasets/abhisu30/transformer-hackathon-leaderboard)

## Quick Start (3 Steps)

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Run the hackathon pipeline (60 minutes on T4 GPU)
python run_hackathon.py --time 60

# 3. Follow the prompts!
```

That's it! The script will automatically:
- Download **TinyStories dataset** (~100 MB, 40K stories)
- Train your model for **60 minutes**
- Evaluate performance (perplexity, tokens/sec)
- Upload to the leaderboard

**Expected Results (T4 GPU):**
- Final Perplexity: **10-18**
- Training Speed: **1,500-2,000 tokens/sec**
- Total Tokens Processed: **~6-8 million**

---

## ü§ñ Chat with Your Model (ChatGPT-Style!)

After training, interact with your model in real-time:

```bash
# Start interactive chat
python chat.py
```

**What you'll see:**
```
You: Once upon a time
ü§ñ Model: there was a little girl named Lily. She loved to play with her toys...

You: A boy named
ü§ñ Model: Tom went to the park. He saw a big tree and wanted to climb it...
```

**Built-in features:**
- üí° 10 prompt suggestions for TinyStories
- üéõÔ∏è Adjustable creativity (temperature)
- üèÜ Auto-loads your best checkpoint
- ‚å®Ô∏è Type `help` for suggestions, `quit` to exit

**Customize generation:**
```bash
python chat.py --temperature 1.0    # More creative
python chat.py --max-tokens 300     # Longer responses
```

üìñ **Full guide:** [CHAT_GUIDE.md](CHAT_GUIDE.md)

---

## üìÅ Repository Structure

```
Transformer_Repo-/
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ COLAB_GUIDE.md           # Google Colab instructions
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ config.py                 # Hyperparameters (MODIFY THIS!)
‚îÇ
‚îú‚îÄ‚îÄ model/                    # üß† Transformer components
‚îÇ   ‚îú‚îÄ‚îÄ Notebooks (for learning):
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Attention.ipynb        # ‚ú® Refactored with practice cells
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Embeddings.ipynb       # ‚ú® Refactored with practice cells
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FeedForward.ipynb      # ‚ú® Refactored with practice cells
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Decoder.ipynb          # Decoder blocks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Transformer.ipynb      # Complete GPT model
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Python files (used by training):
‚îÇ       ‚îú‚îÄ‚îÄ embeddings.py          # Token + positional embeddings
‚îÇ       ‚îú‚îÄ‚îÄ attention.py           # Multi-head self-attention
‚îÇ       ‚îú‚îÄ‚îÄ feedforward.py         # Feed-forward network
‚îÇ       ‚îú‚îÄ‚îÄ decoder_block.py       # Decoder layer (GPT-style)
‚îÇ       ‚îú‚îÄ‚îÄ decoder.py             # Full decoder stack
‚îÇ       ‚îî‚îÄ‚îÄ transformer.py         # Complete GPT model
‚îÇ
‚îú‚îÄ‚îÄ data/                     # üìö Data handling
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py          # Character-level tokenizer
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py            # Dataset loading (TinyStories 40K)
‚îÇ   ‚îî‚îÄ‚îÄ Data_Processing.ipynb # Data exploration notebook
‚îÇ
‚îú‚îÄ‚îÄ utils/                    # üîß Utilities
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py            # Evaluation metrics
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint.py         # Save/load models
‚îÇ   ‚îî‚îÄ‚îÄ huggingface_upload.py # Leaderboard integration
‚îÇ
‚îú‚îÄ‚îÄ train.py                  # Training script
‚îú‚îÄ‚îÄ evaluate.py               # Evaluation script
‚îú‚îÄ‚îÄ generate.py               # Text generation
‚îú‚îÄ‚îÄ chat.py                   # ü§ñ Interactive chat interface (NEW!)
‚îî‚îÄ‚îÄ run_hackathon.py          # üèÜ Main hackathon script
```

---

## üèóÔ∏è Architecture Overview

**GPT-Style Decoder-Only Transformer** (No Encoder!)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GPT-Style Transformer                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Input: "The cat sat on the"                                    ‚îÇ
‚îÇ              ‚Üì                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  TOKEN EMBEDDING + POSITIONAL ENCODING                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Convert tokens to vectors                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Add position information (sinusoidal)                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ              ‚Üì                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ              DECODER BLOCK (√ó6)                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  MASKED MULTI-HEAD SELF-ATTENTION                  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Q, K, V projections                             ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ 8 attention heads                               ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Causal masking (can only see past)              ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              ‚Üì + Residual + LayerNorm                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  FEED-FORWARD NETWORK                              ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Linear(512 ‚Üí 2048)                              ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ GELU activation                                 ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ Linear(2048 ‚Üí 512)                              ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ              ‚Üì + Residual + LayerNorm                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ              ‚Üì                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  OUTPUT PROJECTION                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Linear(512 ‚Üí vocab_size)                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Softmax ‚Üí probability distribution                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ              ‚Üì                                                   ‚îÇ
‚îÇ  Output: "mat" (predicted next token)                           ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Default Configuration:
‚Ä¢ d_model = 512 (embedding dimension)
‚Ä¢ n_heads = 8 (attention heads)
‚Ä¢ n_layers = 6 (decoder blocks)
‚Ä¢ d_ff = 2048 (feed-forward dimension)
‚Ä¢ max_seq_len = 128 (context window)
‚Ä¢ vocab_size = ~65 (character-level)
‚Ä¢ Parameters = ~10M
‚Ä¢ Dataset = TinyStories (40K stories, ~100 MB)
‚Ä¢ Training Time = 60 minutes (T4 GPU)
```

---

## üìì Learning with Notebooks

The repository includes **refactored Jupyter notebooks** for hands-on learning:

### ‚ú® Refactored Notebooks (with Practice Cells)

1. **Attention.ipynb** - Multi-head self-attention
   - Individual cells for each function
   - Practice cells after each implementation
   - Learn by doing!

2. **Embeddings.ipynb** - Token and positional embeddings
   - TokenEmbedding, PositionalEncoding, TransformerEmbedding
   - Step-by-step breakdown

3. **FeedForward.ipynb** - Position-wise feed-forward networks
   - Simple but crucial component
   - Practice implementing from scratch

### üìö Complete Notebooks

4. **Decoder.ipynb** - Decoder blocks and stack
5. **Transformer.ipynb** - Complete GPT model

**How to Use:**
- Open notebooks in Jupyter or Google Colab
- Run cells sequentially
- Try implementing functions in practice cells
- Compare with reference implementations

## üöÄ One-Click Colab Setup

Run this cell to set everything up instantly!

```python
# üöÄ ONE-CLICK SETUP
# Run this cell to set everything up!

# Check GPU
import torch
assert torch.cuda.is_available(), "‚ö†Ô∏è Enable GPU: Runtime > Change runtime type > GPU"

# Clone repository
!git clone https://github.com/abhishekadile/Transformer_Repo-.git
%cd Transformer_Repo-

# Install dependencies
!pip install -r requirements.txt

# Start Hackathon! (5 minutes on MacbookPro)
!python run_hackathon.py --time 5 --name Glove 
```

---

## ü§ñ Interactive Chat with Your Model

Chat with your trained model in real-time using our ChatGPT-like interface.

```python
# After training, chat with your model!
!python chat.py
```

### Features

- üí¨ **ChatGPT-like interface** - Interactive terminal chat
- üí° **Smart prompt suggestions** - Tailored for TinyStories
- üèÜ **Auto-loads best checkpoint** - Uses `best.pt` automatically
- ‚ö° **Real-time generation** - See your model's creativity!

### Example Session

```
You: Once upon a time
ü§ñ Model: there was a little girl named Lily. She loved to play with her toys...

You: A boy named
ü§ñ Model: Tom went to the park. He saw a big tree and wanted to climb it...
```

### Prompt Suggestions

The chat interface includes 10 built-in prompts perfect for TinyStories:
- "Once upon a time"
- "One day, a little"
- "There was a"
- "A boy named" / "A girl named"
- And more!

### Custom Settings

```python
# Longer responses
!python chat.py --max-tokens 300

# More creative (higher temperature)
!python chat.py --temperature 1.0

# More focused (lower temperature)
!python chat.py --temperature 0.5
```

üìñ **Full guide:** See [CHAT_GUIDE.md](CHAT_GUIDE.md) for detailed usage and tips!

---


## üîß Optimization Ideas

Here are proven techniques to improve your model:

### Easy Wins üü¢

```python
# config.py
# 1. Enable mixed precision (2x speedup on modern GPUs!)
config.training.use_amp = True

# 2. Increase batch size if memory allows
config.training.batch_size = 32

# 3. Try different learning rates
config.training.learning_rate = 1e-3  # or 5e-4
```

### Medium Difficulty üü°

```python
# 1. Gradient accumulation for larger effective batch size
config.training.gradient_accumulation_steps = 4

# 2. Bigger model (if GPU memory allows)
config.model.d_model = 768
config.model.n_layers = 8

# 3. Adjust dataset size
config.data.max_stories = 50000  # More data = better model
```

### Advanced üî¥

```python
# 1. Implement KV caching for faster generation
# See model/transformer.py - GPTModel.generate()

# 2. Try different attention mechanisms
# Flash Attention, Linear Attention, etc.

# 3. Experiment with different architectures
# Add cross-attention, use different normalization, etc.
```

---

## üìä Dataset Information

**TinyStories Dataset**
- **Source:** Hugging Face `roneneldan/TinyStories`
- **Default Size:** 40,000 stories (~100 MB)
- **Format:** Simple children's stories
- **Tokenization:** Character-level (vocab size ~65)

**Why TinyStories?**
- Fast to download and process
- Simple language = easier to learn
- Good for 60-minute training sessions
- Produces coherent output

**Adjusting Dataset Size:**
```python
# config.py
config.data.max_stories = 50000  # Increase for better quality
# Training time scales linearly with dataset size
```

---

## üéØ Training Tips

### For Google Colab (T4 GPU)

```bash
# Recommended settings for 60-minute training
python run_hackathon.py --time 60 --batch-size 16 --use-amp
```

**Expected Performance:**
- Perplexity: 10-18
- Speed: 1,500-2,000 tokens/sec
- Total tokens: ~6-8 million

### For Local Training (CPU)

```bash
# Slower but works!
python run_hackathon.py --time 10 --batch-size 8
```

**Expected Performance:**
- Perplexity: 15-25 (less training time)
- Speed: 500-1,000 tokens/sec
- Use smaller dataset for faster iteration

---

## üêõ Troubleshooting

### Common Issues

**1. Out of Memory**
```python
# Reduce batch size
config.training.batch_size = 8

# Or reduce model size
config.model.d_model = 256
config.model.n_layers = 4
```

**2. Slow Training**
```python
# Enable mixed precision
config.training.use_amp = True

# Reduce dataset size
config.data.max_stories = 20000
```

**3. Poor Generation Quality**
```python
# Train longer
python run_hackathon.py --time 120

# Use more data
config.data.max_stories = 60000

# Bigger model
config.model.d_model = 768
```

---

## üìö Additional Resources

- **Original Paper:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- **GPT Paper:** [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- **Illustrated Transformer:** [Jay Alammar's Blog](http://jalammar.github.io/illustrated-transformer/)
- **TinyStories Paper:** [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759)

---

## ü§ù Contributing

Found a bug? Have an optimization idea? PRs welcome!

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-optimization`)
3. Commit your changes (`git commit -m 'Add amazing optimization'`)
4. Push to the branch (`git push origin feature/amazing-optimization`)
5. Open a Pull Request

---

## üìù License

MIT License - feel free to use this for learning and teaching!

---

## üôè Acknowledgments

- TinyStories dataset by Microsoft Research
- Inspired by Andrej Karpathy's nanoGPT
- Built for educational purposes

---

**Happy Hacking! üöÄ**

Questions? Open an issue or reach out on the leaderboard discussion!
